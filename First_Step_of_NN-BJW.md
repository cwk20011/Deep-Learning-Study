# 신경망 첫걸음

part1. 신경망의 동작원리

- 분류자 학습하기

[오차 (E) = 목표값 - 실제 출력값]

선형 분류자의 오차와 기울기 매개변수 간의 관계에서부터 오차를 제거하기 위해 얼마만큼 기울기를 조정해야하는가를 알 수 있음

문제점 : 이전의 학습 데이터는 무시하고 <u>최종 학습 데이터에만 fit</u>되어 업데이트

[ 변화율(A) = L( E / x ) ] ~> L = 학습률

**==> 해결책** : <u>학습률(learning rate)</u>을 적용하여 업데이트의 정도를 조정

​	1) 단일 학습 데이터가 학습에 지배적인 영향을 주는 것을 방지
​	2) 데이터의 오류의 영향을 제한

- 뉴런

  - 분계점 (threshold)

  - 활성화 함수 (activation function)

    : 시그모이드함수(= 로지스틱함수) 

  - 계층

|      | 형태 | 수식 |      |
| ---- | ---- | ---- | ---- |
|      |      |      |      |
|      |      |      |      |
|      |      |      |      |



- 행렬곱
  - Dot product (점곱)
  - inner product (내적)

신경망에서 신호를 전달하는 연산은 **행렬곱**을 통해 표현 -> 크기에 상관없이 간결하고, 효율적으로 처리가능



전파법 (forward propagation) : 초기 입력 신호를 가중치와 조합하여, 마지막 계측쪽으로!! 즉, 앞으로 전달 => 하나의 계층에서 다음 계층으로 전파하는데 가중치 이용



역전파 (back propagation) : 각각의 연결이 오차에 영향을 주는 정도에 <u>비례해서 전달</u>

![](/Users/baejiwon/Desktop/스크린샷 2019-04-01 오후 7.33.06.png)

- 가중치

출력 노드가 여러개라고 해서 달라지는 것은 아님

첫번째 출력노드의 오차 [e1 = t1 - o1]

~> e1은 w11과 w21의 가중치 값에 비례해 나뉘어 연결된 노드로 전달

w1을 업데이트 하기 위해 사용되는 e1의 일부 = w21 /( w11 + w21)

==> 출력 계층의 노드들의 오차를 이와 연결된 가중치의 크기에 비례해 나눠서 역전파하고 이를 재조합



- 오차의 역전파

  - 여러 노드에서의 오차의 역전파
  - 다중 계층에서의 오차의 역전파
  - 행렬곱을 이용한 오차의 역전파

- 가중치의 업데이트

  - 가중치 계산

    [ 신경망의 오차 = 가중치의 함수 ]

  - 경사하강법 : 최적의 가중치를 찾기 위해, 작은 발걸음으로 오차함수를 줄여가면서 반복적으로 가중치 개선 (미분을 이용하여 오차의 기울기를 계산)

    => 함수의 최저점을 구하기 위한 좋은 접근 방법

    매개변수가 많은 경우, 데이터의 불완전성 혹은 함수의 불완전 표현의 경우에도 탄력적인 대응

     - Global minimum
     - Local minimum : 국소 최저점의 빠졌을 경우의 해답 (학습률을 확장)

  - 오차함수

    ![](/Users/baejiwon/Desktop/스크린샷 2019-04-01 오후 7.38.55.png)

    -  e<sub> j</sub> : 첫번째 부분의 오차인 (목표값 - 실제값)이 은닉 계층에서 재조합된 역전파 오류

    - i<sub> j</sub> : 두번쨰 시그모이드 부분은 동일하지만, 합부분이 은닉 계층의 노드 j로 들어오는 입력값에 가중치를 적용한 결과 (입력값)

    - o<sub> j</sub> : 첫번째 계층의 노드의 결과값 o<sub> i</sub> 를 계산한것

      

  - 학습률

    가중치는 기울기와 반대 방향으로 진행됨  -> 학습률 인자를 다르게 튜닝하면서 변화의 정도를 조정(오버슈팅을 방지하기 위하여)

    **학습률** : 오버슈팅을 방지하기 위해 변화의 강도를 조정하는 역할 (alpha)

    ![](/Users/baejiwon/Desktop/스크린샷 2019-04-01 오후 7.49.42.png)

    은닉계층과 출력 계층 사이 가중치뿐만 아니라, 입력 계층과 은닉 계층 사이 가중치에도 동일하게 적용됨

    

    가중치 변화의 행렬은 한 계층의 노드 j와 다음 계층의 노드 k를 연결하는 가중치 w<sub> jk</sub> 를 조정하는 역할

    

    - 가중치 업데이트 행렬

  ![](/Users/baejiwon/Desktop/스크린샷 2019-04-01 오후 7.47.45.png)

